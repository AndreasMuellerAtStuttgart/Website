<h1>Project History</h1>
    
    <p>In the following, I'll describe the projects I have taken part in so far. Use the navigation menu below to choose a 
        particular project.
    </p>

    <ul>
        <li>03/2012-08/2013: <a href="#TOPAS">TOPAS project</a> for patent analysis</li>
        <li>09/2013-01/2016: <a href="#ePoetics">ePoetics project</a> for poetics analysis</li>
        <li>08/2016-02/2017: <a href="#In-House">In-house projects</a> at the IT departement of an insurance company</li>
        <li>03/2019-present: Optimisation of a tesseract-based OCR component for a <a href="#OCR">
            project extracting data from PDF-files</a></li>
    </ul>

    <article>
        <h2 id="TOPAS">TOPAS (Tool platform for intelligent Patent Analysis and Summarization)</h2>
            <p>The TOPAS project had as its aim to recognize relevant semantic entities in patents to support intelligent patent analysis 
                and automatic patent summarization. Examples for semantic entities are references to other patents, references to 
                objectives of the invention, references to the figures mentioned in the patent text and many more. While one of the 
                most important goals was to enable automatic summarization, the methods developed were used for more than summarization. 
                I was involved in two tasks:
            </p>
            <ol>
                <li>Recognizing the Structure of Patents Automatically</li>
                <li>Claim-Description Alignment</li>
            </ol>

            <p>From a technical perspective, both tasks consisted of writing analysis components for the 
                <a href="https://gate.ac.uk/">GATE</a> text analysis framework with Java SE. These components then were integrated with 
                components developed by colleagues to form processing pipelines for end-to-end tasks, like automatic summarization. While 
                the results of task 1 provided the input for processes like summarization, task 2 was itself an end-to-end task because 
                its results were useful independent of other processes. In the following, both tasks are described in more detail.
            </p>

            <h3>Recognizing patent structure automatically</h3>
            <p>Patent documents consist of different parts. Two of the most important of those parts are the claims and the description. 
                Claims constitute the legally binding description of the patented invention. In order to not limit the scope of a patent 
                too much claims tend to consist of long, complicated sentences and very general formulations, so that as many potential 
                use cases as possible are covered. They are notoriously hard for humans to read and understand. The description usually 
                clarifies the meaning of the claims, describes relevant background art and potential embodiments of the invention. 
                Descriptions consist of a small number of sections, like "Field of the Invention" or "Summary of the Invention", some of 
                which are optional. My task was to write an analysis component which automatically recognizes the different sections of 
                the description. In some cases this was fairly simple, because the sections were marked with corresponding headlines. 
                The problem was complicated, however, by two properties of descriptions:
            </p>
            
            <ol>
                <li>Sometimes only some of the sections have headlines, but not all</li>
                <li>Sometimes there are no headlines marking the sections</li>
            </ol>

            <p>The second problem made the implementation of an algorithm necessary which took the sequence of paragraph texts the 
                description consists of as input and outputs which paragraphs belong to which section. This algorithm was implemented 
                with simple rules. For example, the paragraphs consituting the description of the drawings the description references 
                consist of a list of the drawings in order from first to last. The rule to recognize the paragraphs was then simply to 
                recognize the shortest sequence of paragraphs were in the first paragraph there is a reference to the first drawing, in 
                the last paragraph there is a reference to the last drawing and the sequence contains at least one reference to every 
                drawing. With similar rules we found that the structure of descriptions could be realiably recognized with a recognition 
                accuracy of around 85%. The first problem is simply handled by combining the results of the algorithm which recognizes 
                the structure based on headlines and the algorithm which recognizes the structure without headlines. 
                <br/>
                For a more detailed summary see the section "Segmentation of patent material" in the 
                    <a href="https://cordis.europa.eu/project/rcn/100384/reporting/en">Final Deliverable</a>.
            </p>

            <h3>Claim-Description Alignment</h3>
            <p>Since the patent description can be seen as a thorough explanation of the patent claims, there are parts in the claims 
                which are described by certain parts in the description. Claim-Segment Alignment is the task of automatically finding 
                the parts in the description which elaborate on parts of the claims. This alignment is useful for browsing claims. For 
                example, if a claim explains a concept in general terms which are difficult to understand, automatically linking to a 
                concrete example of an embodiment of the generel concept can be very helpful for understanding the concept.
                Claim-Segment is very difficult because of a potential vocabulary mismatch between claims and description: The same 
                concept can have one name in the claims and a different name in the description. The problem is compounded by patents 
                using complex, long noun phrases to refer to entities sometimes. When just parts of those noun phrases match it can be 
                difficult to decide whether the concept refered to by two noun phrases are the same. Imagine for example 
                "the battery of a motor" occurring in the claims and "the battery of the motor of vehicle 1" occurring in the description. 
                Do those two phrases refer to the same entity? It is hard, if not impossible to decide just by looking at those two phrases 
                alone. Clearly, we need to take the context of the phrases into account. Another problem is that each claim 
                is just one long, complex sentence potentially consisting of multiple parts, each describing different aspects of the 
                invention. Automatically finding those multiple parts turned out to be, at least in some cases, a difficult problem.
            </p>

            <p>We used a similarity-based approach for Claim-Description Alignment. Among other types of alignments, we aligned 
                sentences in the description to aspects of the claims. An aspect is a description of a part of the functionality or 
                composition of an invention contained as a sub-sentence in a claim. To implememt this alignment we computed the 
                similarity between the phrases refering to elements of the invention in the aspect and the sentence. We then combined 
                this similarity with the lexical similarity (roughly speaking what percentage of words they share) of the aspect and the 
                sentence. The combined similarity then made up the similarity score between aspect and sentence. This type of similarity 
                score could be used in a claim-browsing application by letting the user manipulate a similarity threshold, where all 
                aspect-sentence pairs with a similarity above the threshold are highlighted. By manipulating this score at runtime, 
                a claim-browsing application could visualize alignments at different similarity thresholds, leaving the ultimate decision 
                of what similarity value a sentence-aspect pair has to have to count as aligned to the user.
                <br/>

                For more details about Claim-Description Alignment see the section "Claim â€“ description alignment" in the 
                <a href="https://cordis.europa.eu/project/rcn/100384/reporting/en">Final Deliverable</a>.
            </p>

            <p>During the project I worked closely with the involved team at the 
                <a href="https://www.upf.edu/">Universidad Pompeu Fabra</a> in Spain, resulting in three short visits to Spain for 
                coordinating my work with theirs. I also worked with our three industry partners from Germany, Spain and Italy. 
                To the best of my knowledge at least one of them is actively using the results of the projects. 
            </p>
        </article>

    <article>
        <h2 id="ePoetics">ePoetics</h2>
        <p>The project ePoetics was a collaboration between experts from the humanities, visual analytics and computational linguistics. 
            In this project, the goal was to explore novel techniques for analyzing literary texts, using the example of poetics. 
            Poetics are scientific works about literature, especially poetic forms of literature. My main job in the project was to 
            build novel tools for literary analysis, working closely with the developer from the field of visual analytics and an expert 
            for the literature we used as a testbed. Roughly speaking, I was in charge of building the backend structures and analysis 
            capabilities which would then form the basis for providing access to those structures and capabilities via appropriate 
            visualizations. The biggest success in terms of tools is the VarifocalReader, a tool for analyzing text documents 
            with a hierarchical structure. The main challenges was finding a data structure appropriate for text with multiple 
            structures being hierarchically embedded in other structures and to visualize the results of machine learning methods. 
            A demo of the VarifocalReader can be found 
            <a target="_blank" href="https://www.youtube.com/watch?v=uE--LDVZLe4&feature=youtu.be">here.</a>
        </p>

        <p>Aside from working on tools, I also used machine learning methods to conduct experiments for recognizing expressions and 
            their attributions in texts. For more information about my involvement with the project, see the list of publications on 
            <a href="http://epoetics.visus.uni-stuttgart.de/index8090.html?page_id=115" target = "_blank">the project homepage</a> (
                bottom of the page).
        </p>
    </article>
    <article>
        <h2 id="In-House">Projects with web services as the central theme</h2>
        <p>In the summer of 2016 I began working as a trainee for Java-based web service development at the IT departement of a 
            large insurance company. There I was involved in various projects, with the central theme of either working on data access 
            web services directly or working on a component coordinating calls to data access services. I worked with the following 
            technologies: Spring, Java EE, Eclipse, Maven, Jenkins, SOAP-UI, JIRA and Confluence. My work also involved documenting 
            web service functionalities and communicating with subject matter experts as well as technical people from other 
            departements. Unfortunately I had to leave the company in February of 2017 due to illness, so I only got a taste of all the 
            technologies involved. I still think this taste was invaluable for my growth as a developer, because it was the first time 
            I worked <strong>in</strong> a company. Before this job I had only worked <strong>with</strong> companies, but only in the 
            context of research projects, which was a very different experience from working in a corporate environment.
        </p>
    </article>
    <article>
        <h2 id="OCR">Optimization of tesseract-based OCR component</h2>
        <p>After a long period of rehabilitation I started an internship at the Fraunhofer Institute for Work and Organization 
            in Stuttgart, and that's the position I currently hold. In this position I optimize the OCR component of a system which needs 
            to parse scanned PDF documents and extract important data from the documents. The OCR is based on the tess4j Java wrapper for 
            Tesseract. I work on three aspects: Optimizing the parameters of Tesseract itself, image pre-processing for documents where the 
            OCR yields bad results (e.g. because of shadows on the document) and post-processing to use knowledge about the language 
            contained in those documents to implement automatic corrections and warnings about possible OCR errors. From a technical 
            perspective, it is 
            Java SE programming with specialized libraries, in the context of a Maven project. The development methodology is 
            agile, with small goals being defined for one week. The progress is evaluated every week and goals are set/adjusted based on 
            the progress made in a week. The internship will end in September. I hope to be able to find a position in a company until 
            then. I have some flexibillity regarding this position because I have the option to start with an internship and then 
            transition into a job if it works out for both sides. So if you have a position for which you think I'd be a good fit, 
            don't hesitate to contact me.
        </p>
        <script src="" async defer></script>
    </article>
